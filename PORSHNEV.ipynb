{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PORSHNEV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LKu74HliYHH"
      },
      "source": [
        "Program for final task should be able to provide frequency analysis data for several documents stored in your Colab folder.\n",
        "\n",
        "\n",
        "**Frequencies of tokens (write sorted by frequency result in csv file tokens.csv)**\n",
        "\n",
        "**Frequencies of bigrams (write sorted by frequency result in csv file bigram.csv)**\n",
        "\n",
        "**Frequencies of stems (Porter stemmer) (write sorted by frequency result in csv file stems.csv)**\n",
        "\n",
        "**Frequencies of lemmas (write sorted by frequency result in csv file lemmas.csv)**\n",
        "\n",
        "**Frequencies of parts of speech (write sorted by frequency result in csv file pos.csv)**\n",
        "\n",
        "**Frequencies of sentiment markers (write sorted by frequency result in csv file sentiment.csv)**\n",
        "\n",
        "You will receive additional points +1 If your program reads several document formats (html, TXT, or **txt with different encoding (ANSI, UTF-8)**+1 **If your program Operates with multiple languages (English, Russian, other?)** +1 If put a new specific feature to the program related to Text Anaytics\n",
        "To achieve grade 10 (outstanding) you need in addition to the presented functionality add a complex function like Topic Modelling, complex Sentiment Analysis, ability to generate new texts **(Chat Bot)** or other NLP tasks – for ideas and realization please audit https://www.coursera.org/learn/python-text-mining/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlGVBSiKF6Gg",
        "outputId": "56320158-c5a8-419a-80ad-a8530b2e2337"
      },
      "source": [
        "#checking the encoding of the txt file\n",
        "import chardet    \n",
        "rawdata = open('Text4.txt', \"rb\").read() #read file as a binary data\n",
        "result = chardet.detect(rawdata)\n",
        "charenc = result['encoding']\n",
        "print (charenc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UTF-8-SIG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdgRmsb-A9L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f269bc5-b5c5-47f9-d5d3-7cc09c16949b"
      },
      "source": [
        "#opening the file using the encoding which we found out above\n",
        "f = open('Text4.txt', 'r', encoding=charenc)\n",
        "text1 = f.read()\n",
        "f.close()\n",
        "print(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Нет, ну правда! Это же жизнь, только очень концентрированная и плотно упакованная буквально в несколько дней. И как и в жизни, нужно внимательно отнестись к выбору напарника: если вы на разных волнах, путешествию несдобровать. Зато если на одной – вы в деле: за завтраком можно решить поехать на маршрутке в Кронштадт, купить грузинский лаваш в рандомном киоске на остановке, поверить продавщице из магазина с сомнительным названием «Дагвино» (что? да!) и купить бутылку «редкого сорта красностоп» (вы вообще хоть раз о таком слышали?). Можно, разделив наушники, два часа сидеть на берегу Финского залива, вписаться на самые страшные аттракционы, устроить день книжных магазинов, можно даже ехать в купе около туалета (потому что оно на скидках). Можно всё, и вот эта свобода, наверное, это молодость и есть.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejY4JRWQ0Qq5",
        "outputId": "3ec19233-0011-47e2-d6a1-8d6502018c85"
      },
      "source": [
        "#finding out the language of text\n",
        "!pip install langdetect\n",
        "from langdetect import detect \n",
        "print(detect(text1))\n",
        "language=detect(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\r\u001b[K     |▍                               | 10kB 20.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 24.3MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 15.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 15.8MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 13.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 14.9MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 13.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 12.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 12.4MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 12.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 12.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901kB 12.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931kB 12.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 983kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993194 sha256=0fbf1ebf9945d97ca820ed264c9ff448c147793fa88b1cb91c101caecadda367\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n",
            "ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rh2oI3jlwia",
        "outputId": "02996fc5-995e-4890-f200-e7647ad2d0be"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = nltk.word_tokenize(text1) #broke text into tokens\n",
        "tokens\n",
        "word_tokens = []\n",
        "#for making the text better to analyse we take away everything if it's less than 1 letter (except \"I\" and \"A\")\n",
        "#and we try to get rid of any digits\n",
        "if language=='en':\n",
        "  for i in tokens:\n",
        "    if len(i)>1 and i.isdigit()==False or i.lower()=='i' or i.lower()=='a': \n",
        "      word_tokens.append(i.lower())\n",
        "\n",
        "elif language=='ru':\n",
        "  for i in tokens:\n",
        "    if len(i)>1 and i.isdigit()==False or i.lower()=='я' or i.lower()=='а' or i.lower()=='у' or i.lower()=='и':\n",
        "      word_tokens.append(i.lower())\n",
        "\n",
        "print(word_tokens) \n",
        "#counting the frequency of the tokens\n",
        "word_fd = nltk.FreqDist(word_tokens).most_common()\n",
        "word_fd \n",
        "\n",
        "import csv\n",
        "f = open(\"tokens.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "#writing results of counting     \n",
        "for i in word_fd:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['нет', 'ну', 'правда', 'это', 'же', 'жизнь', 'только', 'очень', 'концентрированная', 'и', 'плотно', 'упакованная', 'буквально', 'несколько', 'дней', 'и', 'как', 'и', 'жизни', 'нужно', 'внимательно', 'отнестись', 'выбору', 'напарника', 'если', 'вы', 'на', 'разных', 'волнах', 'путешествию', 'несдобровать', 'зато', 'если', 'на', 'одной', 'вы', 'деле', 'за', 'завтраком', 'можно', 'решить', 'поехать', 'на', 'маршрутке', 'кронштадт', 'купить', 'грузинский', 'лаваш', 'рандомном', 'киоске', 'на', 'остановке', 'поверить', 'продавщице', 'из', 'магазина', 'сомнительным', 'названием', 'дагвино', 'что', 'да', 'и', 'купить', 'бутылку', 'редкого', 'сорта', 'красностоп', 'вы', 'вообще', 'хоть', 'раз', 'таком', 'слышали', 'можно', 'разделив', 'наушники', 'два', 'часа', 'сидеть', 'на', 'берегу', 'финского', 'залива', 'вписаться', 'на', 'самые', 'страшные', 'аттракционы', 'устроить', 'день', 'книжных', 'магазинов', 'можно', 'даже', 'ехать', 'купе', 'около', 'туалета', 'потому', 'что', 'оно', 'на', 'скидках', 'можно', 'всё', 'и', 'вот', 'эта', 'свобода', 'наверное', 'это', 'молодость', 'и', 'есть']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khu94Viwt_Gv"
      },
      "source": [
        "from nltk.collocations import *\n",
        "\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(word_tokens, window_size = 2)\n",
        "\n",
        "scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
        "scored\n",
        "\n",
        "f = open(\"bigram.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "\n",
        "for i in scored:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm3bByQJxj_n"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "russianStemmer=SnowballStemmer(\"russian\")\n",
        "\n",
        "if language=='en':\n",
        "  word_stems = []\n",
        "  for i in word_tokens:\n",
        "    word_stems.append(porter.stem(i))\n",
        "\n",
        "elif language=='ru':\n",
        "  word_stems = []\n",
        "  for i in word_tokens:\n",
        "    word_stems.append(russianStemmer.stem(i))\n",
        "\n",
        "else:\n",
        "  print (\"Sorry, but the language is not supported by this program\")\n",
        "    \n",
        "\n",
        "stem_fd = nltk.FreqDist(word_stems).most_common()\n",
        "stem_fd \n",
        "\n",
        "f = open(\"stems.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "    \n",
        "for i in stem_fd:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ6t5MaLzbgQ",
        "outputId": "44c6a886-64d5-4f40-faf8-cfcc63a92c2f"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def wrap_lemmatize(token,tag):\n",
        "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
        "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
        "\n",
        "    if tag in Noun_tags:\n",
        "            return lemmatizer.lemmatize(token,'n') #wordnet.ADJ,  wordnet.NOUN, wordnet.VERB, wordnet.ADV}\n",
        "    elif tag in Verb_tags:\n",
        "            return lemmatizer.lemmatize(token,'v')\n",
        "    else:\n",
        "            return lemmatizer.lemmatize(token)\n",
        "\n",
        "pos_text = nltk.pos_tag(word_tokens)\n",
        "\n",
        "if language=='en':\n",
        "  word_lems = []\n",
        "  for i in pos_text:\n",
        "    word_lems.append(wrap_lemmatize(i[0], i[1])) \n",
        "\n",
        "elif language=='ru':\n",
        "  word_lems = []\n",
        "  for i in word_tokens:\n",
        "    p = morph.parse(i)[0]\n",
        "    word_lems.append(p.normal_form)\n",
        "\n",
        "else:\n",
        "  print (\"Sorry, but the language is not supported by this program\")\n",
        "\n",
        "lems_fd = nltk.FreqDist(word_lems).most_common()\n",
        "lems_fd\n",
        "\n",
        "f = open(\"lemmas.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "\n",
        "for i in lems_fd:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQxZhdI655hI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "febac6b5-0703-425c-8d3a-0f0c6596807c"
      },
      "source": [
        "pos = []\n",
        "\n",
        "if language == 'en':\n",
        "  pos_text = nltk.pos_tag(word_tokens)\n",
        "  for i in pos_text:\n",
        "    pos.append(i[1])\n",
        "\n",
        "elif language == 'ru':\n",
        "  pos_text = nltk.pos_tag(word_tokens, lang='rus')\n",
        "  for i in pos_text:\n",
        "    pos.append(i[1])\n",
        "\n",
        "pos_fd = nltk.FreqDist(pos).most_common()\n",
        "pos_fd\n",
        "\n",
        "f = open(\"pos.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "\n",
        "for i in pos_fd:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aeba8978bfde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ru'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mpos_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rus'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0map_russian_model_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUS_PICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_ru\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_ru')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_oA_7J58bd9",
        "outputId": "3d0fb6c7-79d2-450a-8b84-55166f085b01"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "!pip install folium\n",
        "\n",
        "!pip install dostoevsky\n",
        "!python -m dostoevsky download fasttext-social-network-model\n",
        "\n",
        "from dostoevsky.tokenization import RegexTokenizer\n",
        "tokenizer = RegexTokenizer()\n",
        "from dostoevsky.models import FastTextSocialNetworkModel\n",
        "model = FastTextSocialNetworkModel (tokenizer=tokenizer)\n",
        "messages = word_tokens\n",
        "\n",
        "def classify_compound(text, threshold=0.33):\n",
        "     \n",
        "    # initialize VADER\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "     \n",
        "    # Calling the polarity_scores method on sid and passing in the text\n",
        "    # outputs a dictionary with negative, neutral, positive, and compound scores for the input text\n",
        "    scores = sid.polarity_scores(text)\n",
        "\n",
        "    # get compound score\n",
        "    score = scores['compound']\n",
        "     \n",
        "    # translate the score into the corresponding input according to the threshold\n",
        "    if score <= -threshold: return 'Negative'\n",
        "    elif score >= threshold: return 'Positive'\n",
        "    else: return 'Neutral'\n",
        "\n",
        "if language=='en':\n",
        "  sentiments = []\n",
        "  for i in word_tokens:\n",
        "    sentiments.append(classify_compound(i))\n",
        "\n",
        "elif language=='ru':\n",
        "  results = model.predict(messages, k = 1)\n",
        "  sentiments = []\n",
        "  for dict in results:\n",
        "    for key, value in dict.items():\n",
        "      sentiments.append(key)\n",
        "\n",
        "sentiments_fd = nltk.FreqDist(sentiments).most_common()\n",
        "sentiments_fd\n",
        "\n",
        "f = open(\"sentiment.csv\", \"w\") #opening of the file as a variable\n",
        "w = csv.writer(f)\n",
        "\n",
        "for i in sentiments_fd:\n",
        "    w.writerow(i)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.19.4)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n",
            "Requirement already satisfied: dostoevsky in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pytest==5.3.5 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (5.3.5)\n",
            "Requirement already satisfied: pymorphy2==0.8 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (0.8)\n",
            "Requirement already satisfied: fasttext==0.9.1 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (0.9.1)\n",
            "Requirement already satisfied: scikit-learn==0.22.1 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (0.22.1)\n",
            "Requirement already satisfied: razdel==0.4.0 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (0.4.0)\n",
            "Requirement already satisfied: russian-tagsets==0.6 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (0.6)\n",
            "Requirement already satisfied: b-labs-models==2017.8.22 in /usr/local/lib/python3.6/dist-packages (from dostoevsky) (2017.8.22)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (3.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (20.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (8.6.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (1.10.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (0.13.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (0.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->dostoevsky) (20.8)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->dostoevsky) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->dostoevsky) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->dostoevsky) (2.4.393442.3710985)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->dostoevsky) (1.19.4)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->dostoevsky) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1->dostoevsky) (51.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.1->dostoevsky) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.1->dostoevsky) (1.0.0)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.6/dist-packages (from b-labs-models==2017.8.22->dostoevsky) (0.9.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.5->dostoevsky) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.5->dostoevsky) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest==5.3.5->dostoevsky) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "486SpOIJTjUk",
        "outputId": "aba70e9f-cd0c-4edf-bcd0-3617be8548a9"
      },
      "source": [
        "from nltk.chat.util import Chat \n",
        "from nltk.chat.util import reflections\n",
        "\n",
        "set_pairs = [\n",
        "    [\n",
        "        r\"my name is (.*)\",\n",
        "        [\"Hello %1, How are you today ?\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"hi|hey|hello\",\n",
        "        [\"Hello\", \"Hey there\",]\n",
        "    ], \n",
        "    [\n",
        "        r\"what is your name?\",\n",
        "        [\"You can call me a chatbot Dora\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"how are you ?\",\n",
        "        [\"I am fine, thank you! How can i help you?\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"I am fine, thank you\",\n",
        "        [\"great to hear that, how can i help you?\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"how can I help you? \",\n",
        "        [\"I wonder if you can help me\"]\n",
        "    ],\n",
        "    [\n",
        "        r\"i'm (.*) doing good\",\n",
        "        [\"That's great to hear\",\"How can i help you?:)\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"What should I watch on Youtube?\",\n",
        "        [\"There is channel of Yuri Dud. Also you can watch some interviews of Irina Shihman\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"thanks for the suggestion. Are they interesting?\",\n",
        "        [\"Sure, they have the best guests on russian Youtube!\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"What should I read?\",\n",
        "        [\"Oh, I think, you should read Russian literature. It is the best!\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"Really?\",\n",
        "        [\"Sure, you should try it!\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"(.*) thank you so much!\",\n",
        "        [\"I am happy to help\", \"No problem, you're welcome\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"you are useless\",\n",
        "        [\"Sorry that I didn't help you. Try to contact my authors\",]\n",
        "    ],\n",
        "    [\n",
        "        r\"quit\",\n",
        "    [\"Bye, take care. See you soon :) \"]\n",
        "],\n",
        "]\n",
        "\n",
        "def chatbot():\n",
        "        print(\"Hi, I'm Dora, the chatbot you made\") \n",
        "\n",
        "chat = Chat(set_pairs, reflections)\n",
        "print(chat)\n",
        "\n",
        "chat.converse()\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<nltk.chat.util.Chat object at 0x7fae7d09ce90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntgUour-c_P6",
        "outputId": "11ad2a90-cb49-4e07-e802-72461d0e7395"
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def classify_compound(text, threshold=0.33):\n",
        "     \n",
        "    # initialize VADER\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "     \n",
        "    # Calling the polarity_scores method on sid and passing in the text\n",
        "    # outputs a dictionary with negative, neutral, positive, and compound scores for the input text\n",
        "    scores = sid.polarity_scores(text)\n",
        "\n",
        "    # get compound score\n",
        "    score = scores['compound']\n",
        "     \n",
        "    # translate the score into the corresponding input according to the threshold\n",
        "    if score <= -threshold: return 'Negative'\n",
        "    elif score >= threshold: return 'Positive'\n",
        "    else: return 'Neutral'\n",
        "\n",
        "message = input('Write something here: ')\n",
        "\n",
        "def getReply(message):\n",
        "  if 'hi' in message:\n",
        "     reply = \"Hello! My name is Dora. Can I help you?\"\n",
        "  elif 'how are you' in message:\n",
        "     reply = \"I am fine. What about you?\"\n",
        "  elif 'What should I read' in message:\n",
        "     reply = 'Oh, I think, you should read Russian literature. It is the best!'\n",
        "  elif classify_compound(message)=='Negative':\n",
        "     reply = \"Oh, I see you are in a bad mood. I am sorry\"\n",
        "  elif classify_compound(message)=='Positive':\n",
        "     reply = \"Nice to see you in a good mood\"\n",
        "  else:\n",
        "     reply = \"I got it. You are awesome!\"\n",
        "  print('Bot: ' + reply)\n",
        "\n",
        "while ('bye' not in message):\n",
        "    getReply(message)\n",
        "    message = input('Write something here: ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Write something here: Hi\n",
            "Bot: I got it. You are awesome!\n",
            "Write something here: hi\n",
            "Bot: Hello! My name is Dora. Can I help you?\n",
            "Write something here: What should I read\n",
            "Bot: Oh, I think, you should read Russian literature. It is the best!\n",
            "Write something here: Loser\n",
            "Bot: Oh, I see you are in a bad mood. I am sorry\n",
            "Write something here: Nice\n",
            "Bot: Nice to see you in a good mood\n",
            "Write something here: Good\n",
            "Bot: Nice to see you in a good mood\n",
            "Write something here: Bye\n",
            "Bot: I got it. You are awesome!\n",
            "Write something here: bye\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}